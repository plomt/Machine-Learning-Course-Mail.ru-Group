{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"HW6_DE\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RumFnRay9oEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38eaf43-b0fe-4021-9c9c-e845e505e83e"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 69kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 14.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=2a65de1c592e3c8160819e2565e3ea787a63883602294177a04cfd2246fab0b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u292-b10-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u292-b10-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68_AeFo49pEU"
      },
      "source": [
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftRUvQn89-yC"
      },
      "source": [
        "id='1f_9EbnywCj35EBUA32sueigxBjBJwALr'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('War and Peace by Leo Tolstoy (ru).txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpR8DQxm-IRN"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6qPAe12-Q19"
      },
      "source": [
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E-RdW9y-U1o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "a008ae8a-d5e4-4ed2-cf51-328aed1371a1"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8abf60086955:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ffaa5c60210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQRHs49qdDeU"
      },
      "source": [
        "*`ngrok может отработать не с первого раза, повторите при необходимости.`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6LU3RGy-Yag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3617fdf-d21a-452d-d33b-52dbe49deac7"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-31 15:41:22--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.193.189.47, 34.239.208.97, 3.220.114.126, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.193.189.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  4.76MB/s    in 2.8s    \n",
            "\n",
            "2021-05-31 15:41:25 (4.76 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "https://21ec7b7746cc.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgte5waWKrpd"
      },
      "source": [
        "Подсчитай кол-во слов в документе \"War and Peace by Leo Tolstoy (ru).txt\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7AYXXsTKqjm"
      },
      "source": [
        "# код\n",
        "tokens = sc.textFile(\"War and Peace by Leo Tolstoy (ru).txt\").flatMap(lambda line: line.split(\" \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD-f3S_VAZXl"
      },
      "source": [
        "word_counts = tokens.map(lambda word: ('total', 1)).reduceByKey(lambda w1, w2: w1 + w2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcgbqcznB-8J",
        "outputId": "82fe12c1-21f8-4cca-a144-e21cbb5dbe6c"
      },
      "source": [
        "collect_words = word_counts.collect()\n",
        "collect_words[0][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "493897"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJgmIgCzLJTo"
      },
      "source": [
        "id='13yfAoONwq4rS5XrTv3IrcqcFcdgfvK9V'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('mnist-digits-train.txt')\n",
        "\n",
        "id='1VE_9x0LQvOJpHXbXp_RMPl3Q4wRUuOok'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('mnist-digits-test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lPUUuQsQV4F"
      },
      "source": [
        "training = spark.read.format(\"libsvm\") \\\n",
        "  .option(\"numFeatures\", \"784\") \\\n",
        "  .load(\"./mnist-digits-train.txt\")\n",
        "test = spark.read.format(\"libsvm\") \\\n",
        "  .option(\"numFeatures\", \"784\") \\\n",
        "  .load(\"./mnist-digits-test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSU-TTUvNaON"
      },
      "source": [
        "Необходимо обучить модель используя Spark MLlib (модель на ваш выбор, например Decision Tree) и получить accuracy.\n",
        "Подробнее тут: https://spark.apache.org/docs/latest/ml-classification-regression.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs2KoKObMLHn"
      },
      "source": [
        "Разбиваем тренировочную выборку на train и test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlWEMmdFI_OJ"
      },
      "source": [
        "(trainingData, testData) = training.randomSplit([0.7, 0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2gJEnQFMWgo"
      },
      "source": [
        "задаем алгоритм. В данном случае используем LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex7_KbTEI_Em"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "stages = []\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", regParam=0.001)\n",
        "stages += [lr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYHGxVx_M18o"
      },
      "source": [
        "Создаем pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Qp2L6GI-6T"
      },
      "source": [
        "pipeline = Pipeline(stages=stages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJUdYxnASYv0"
      },
      "source": [
        "Задаем сетку параметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTQglxSMZUsD"
      },
      "source": [
        "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [10, 1.0, 0.1, 0.01, 0.001]).build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBIUL8FSb6N"
      },
      "source": [
        "Проводим кросс-валидацию"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRBznEHZZncs"
      },
      "source": [
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(),\n",
        "                          numFolds=5)\n",
        "cvModel = crossval.fit(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlPhGalHS9Qr"
      },
      "source": [
        "Смотрим лучшие параметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHOIBTffRgUB",
        "outputId": "0976fa88-601e-43d8-e2d1-33b329bf028a"
      },
      "source": [
        "cvModel.bestModel.stages[-1]._java_obj.getRegParam()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNTZ5WcaTdDV"
      },
      "source": [
        "Предиктим"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfjICsgwNfaD"
      },
      "source": [
        "prediction = cvModel.transform(testData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HevKOB5JTf-k"
      },
      "source": [
        "Считаем метрику"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcGLGsv7Qf5d",
        "outputId": "8f3042c2-04da-4814-b44a-3a11bf4275d0"
      },
      "source": [
        "labelAndPred = prediction.select('label', 'prediction')\n",
        "metrics_multi = MulticlassMetrics(labelAndPred.rdd.map(tuple))\n",
        "acc = metrics_multi.accuracy\n",
        "acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9224569205113953"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT4MH92BXXVB"
      },
      "source": [
        "Теперь обучим модель на полном наборе данных и сделаем предсказание"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuU_er5zUv72"
      },
      "source": [
        "model = pipeline.fit(training)\n",
        "\n",
        "predictions = model.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_9T1lfWXlJU"
      },
      "source": [
        "Выберем столбец prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeIHN1MRXpDa",
        "outputId": "e247f9b9-738f-4919-8518-f09c05e7f2cc"
      },
      "source": [
        "y_pred = predictions.select('prediction')\n",
        "y_pred.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|prediction|\n",
            "+----------+\n",
            "|       7.0|\n",
            "|       2.0|\n",
            "|       1.0|\n",
            "|       0.0|\n",
            "|       4.0|\n",
            "|       1.0|\n",
            "|       4.0|\n",
            "|       9.0|\n",
            "|       6.0|\n",
            "|       9.0|\n",
            "|       0.0|\n",
            "|       6.0|\n",
            "|       9.0|\n",
            "|       0.0|\n",
            "|       1.0|\n",
            "|       5.0|\n",
            "|       9.0|\n",
            "|       7.0|\n",
            "|       3.0|\n",
            "|       4.0|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv-s45P8Oy8Q"
      },
      "source": [
        "Тут лежат данные для решения задач на sql. Необходимо написать код который формирует из данных датафреймы и используя sqlContext написать sql код с решением. Приветствуется наличие нескольких вариантов решения задачи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKMztZ2eOOrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348359e3-01a6-4aa4-df34-ce9ade67876d"
      },
      "source": [
        "id='1kUIrskM0zNH8u71G9M1BkHjRQYxvgAvh'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('data.zip')\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/regions.csv        \n",
            "  inflating: data/departments.csv    \n",
            "  inflating: data/jobs.csv           \n",
            "  inflating: data/locations.csv      \n",
            "  inflating: data/country.csv        \n",
            "  inflating: data/employees.csv      \n",
            "  inflating: data/job_history.csv    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2if5f3dbCp7g"
      },
      "source": [
        "regions = spark.read.csv('data/regions.csv', header=True, sep='\\t')\n",
        "regions.createOrReplaceTempView(\"regions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxI4KiZMDUpv",
        "outputId": "8e5f0892-81ee-431b-c641-4cecaba20cc5"
      },
      "source": [
        "regions.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------------------+\n",
            "|REGION_ID|         REGION_NAME|\n",
            "+---------+--------------------+\n",
            "|        1|              Europe|\n",
            "|        2|            Americas|\n",
            "|        3|                Asia|\n",
            "|        4|Middle East and A...|\n",
            "+---------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXIF8JnZGTrB"
      },
      "source": [
        "dep = spark.read.csv('data/departments.csv', header=True, sep='\\t')\n",
        "dep.createOrReplaceTempView(\"department\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7aUxm1QIraW",
        "outputId": "61b622cb-8389-4fa2-bc0e-86ef586240fd"
      },
      "source": [
        "dep.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+--------------------+----------+-----------+\n",
            "|DEPARTMENT_ID|     DEPARTMENT_NAME|MANAGER_ID|LOCATION_ID|\n",
            "+-------------+--------------------+----------+-----------+\n",
            "|           10|      Administration|       200|       1700|\n",
            "|           20|           Marketing|       201|       1800|\n",
            "|           30|          Purchasing|       114|       1700|\n",
            "|           40|     Human Resources|       203|       2400|\n",
            "|           50|            Shipping|       121|       1500|\n",
            "|           60|                  IT|       103|       1400|\n",
            "|           70|    Public Relations|       204|       2700|\n",
            "|           80|               Sales|       145|       2500|\n",
            "|           90|           Executive|       100|       1700|\n",
            "|          100|             Finance|       108|       1700|\n",
            "|          110|          Accounting|       205|       1700|\n",
            "|          120|            Treasury|      null|       1700|\n",
            "|          130|       Corporate Tax|      null|       1700|\n",
            "|          140|  Control And Credit|      null|       1700|\n",
            "|          150|Shareholder Services|      null|       1700|\n",
            "|          160|            Benefits|      null|       1700|\n",
            "|          170|       Manufacturing|      null|       1700|\n",
            "|          180|        Construction|      null|       1700|\n",
            "|          190|         Contracting|      null|       1700|\n",
            "|          200|          Operations|      null|       1700|\n",
            "+-------------+--------------------+----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IleHZVH_EIVc"
      },
      "source": [
        "jobs = spark.read.csv('data/jobs.csv', header=True, sep='\\t')\n",
        "jobs.createOrReplaceTempView(\"jobs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug4wZOcwI0hA",
        "outputId": "210cf5df-019c-4120-a357-25f523bfe155"
      },
      "source": [
        "jobs.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+----------+----------+\n",
            "|    JOB_ID|           JOB_TITLE|MIN_SALARY|MAX_SALARY|\n",
            "+----------+--------------------+----------+----------+\n",
            "|   AD_PRES|           President|     20080|     40000|\n",
            "|     AD_VP|Administration Vi...|     15000|     30000|\n",
            "|   AD_ASST|Administration As...|      3000|      6000|\n",
            "|    FI_MGR|     Finance Manager|      8200|     16000|\n",
            "|FI_ACCOUNT|          Accountant|      4200|      9000|\n",
            "|    AC_MGR|  Accounting Manager|      8200|     16000|\n",
            "|AC_ACCOUNT|   Public Accountant|      4200|      9000|\n",
            "|    SA_MAN|       Sales Manager|     10000|     20080|\n",
            "|    SA_REP|Sales Representative|      6000|     12008|\n",
            "|    PU_MAN|  Purchasing Manager|      8000|     15000|\n",
            "|  PU_CLERK|    Purchasing Clerk|      2500|      5500|\n",
            "|    ST_MAN|       Stock Manager|      5500|      8500|\n",
            "|  ST_CLERK|         Stock Clerk|      2008|      5000|\n",
            "|  SH_CLERK|      Shipping Clerk|      2500|      5500|\n",
            "|   IT_PROG|          Programmer|      4000|     10000|\n",
            "|    MK_MAN|   Marketing Manager|      9000|     15000|\n",
            "|    MK_REP|Marketing Represe...|      4000|      9000|\n",
            "|    HR_REP|Human Resources R...|      4000|      9000|\n",
            "|    PR_REP|Public Relations ...|      4500|     10500|\n",
            "+----------+--------------------+----------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO2KJ-KyE1nm"
      },
      "source": [
        "locations = spark.read.csv('data/locations.csv', header=True, sep='\\t')\n",
        "locations.createOrReplaceTempView(\"locations\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8gPl6aII9Re",
        "outputId": "480b9748-5df9-4ed9-f4eb-c69525902d8c"
      },
      "source": [
        "locations.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+-----------+-------------------+----------------+----------+\n",
            "|LOCATION_ID|      STREET_ADDRESS|POSTAL_CODE|               CITY|  STATE_PROVINCE|COUNTRY_ID|\n",
            "+-----------+--------------------+-----------+-------------------+----------------+----------+\n",
            "|       1000|1297 Via Cola di Rie|      00989|               Roma|            null|        IT|\n",
            "|       1100|93091 Calle della...|      10934|             Venice|            null|        IT|\n",
            "|       1200|    2017 Shinjuku-ku|       1689|              Tokyo|Tokyo Prefecture|        JP|\n",
            "|       1300|     9450 Kamiya-cho|       6823|          Hiroshima|            null|        JP|\n",
            "|       1400| 2014 Jabberwocky Rd|      26192|          Southlake|           Texas|        US|\n",
            "|       1500| 2011 Interiors Blvd|      99236|South San Francisco|      California|        US|\n",
            "|       1600|      2007 Zagora St|      50090|    South Brunswick|      New Jersey|        US|\n",
            "|       1700|     2004 Charade Rd|      98199|            Seattle|      Washington|        US|\n",
            "|       1800|     147 Spadina Ave|    M5V 2L7|            Toronto|         Ontario|        CA|\n",
            "|       1900|     6092 Boxwood St|    YSW 9T2|         Whitehorse|           Yukon|        CA|\n",
            "|       2000| 40-5-12 Laogianggen|     190518|            Beijing|            null|        CN|\n",
            "|       2100|  1298 Vileparle (E)|     490231|             Bombay|     Maharashtra|        IN|\n",
            "|       2200|12-98 Victoria St...|       2901|             Sydney| New South Wales|        AU|\n",
            "|       2300|  198 Clementi North|     540198|          Singapore|            null|        SG|\n",
            "|       2400|      8204 Arthur St|       null|             London|            null|        UK|\n",
            "|       2500|Magdalen Centre, ...|    OX9 9ZB|             Oxford|          Oxford|        UK|\n",
            "|       2600|   9702 Chester Road|09629850293|          Stretford|      Manchester|        UK|\n",
            "|       2700|Schwanthalerstr. ...|      80925|             Munich|         Bavaria|        DE|\n",
            "|       2800|Rua Frei Caneca 1...|  01307-002|          Sao Paulo|       Sao Paulo|        BR|\n",
            "|       2900|20 Rue des Corps-...|       1730|             Geneva|          Geneve|        CH|\n",
            "+-----------+--------------------+-----------+-------------------+----------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nPj00oMFa4T"
      },
      "source": [
        "country = spark.read.csv('data/country.csv', header=True, sep='\\t')\n",
        "country.createOrReplaceTempView(\"country\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7RFrPSDJKvd",
        "outputId": "34518cd6-36fc-48e2-8d3d-97a8008c7f13"
      },
      "source": [
        "country.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------+---------+\n",
            "|COUNTRY_ID|COUNTRY_NAME|REGION_ID|\n",
            "+----------+------------+---------+\n",
            "|        AR|   Argentina|        2|\n",
            "|        AU|   Australia|        3|\n",
            "|        BE|     Belgium|        1|\n",
            "|        BR|      Brazil|        2|\n",
            "|        CA|      Canada|        2|\n",
            "|        CH| Switzerland|        1|\n",
            "|        CN|       China|        3|\n",
            "|        DE|     Germany|        1|\n",
            "|        DK|     Denmark|        1|\n",
            "|        EG|       Egypt|        4|\n",
            "|        FR|      France|        1|\n",
            "|        IL|      Israel|        4|\n",
            "|        IN|       India|        3|\n",
            "|        IT|       Italy|        1|\n",
            "|        JP|       Japan|        3|\n",
            "|        KW|      Kuwait|        4|\n",
            "|        ML|    Malaysia|        3|\n",
            "|        MX|      Mexico|        2|\n",
            "|        NG|     Nigeria|        4|\n",
            "|        NL| Netherlands|        1|\n",
            "+----------+------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_tisy9gF1E7"
      },
      "source": [
        "emp = spark.read.csv('data/employees.csv', header=True, sep='\\t')\n",
        "emp.createOrReplaceTempView(\"employees\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c35roZ8dJYa7",
        "outputId": "9df08161-91c0-485a-843f-e38c64a02137"
      },
      "source": [
        "emp.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----------+----------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
            "|EMPLOYEE_ID| FIRST_NAME| LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
            "+-----------+-----------+----------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
            "|        100|     Steven|      King|   SKING|515.123.4567| 17.06.03|   AD_PRES| 24000|          null|      null|           90|\n",
            "|        101|      Neena|   Kochhar|NKOCHHAR|515.123.4568| 21.09.05|     AD_VP| 17000|          null|       100|           90|\n",
            "|        102|        Lex|   De Haan| LDEHAAN|515.123.4569| 13.01.01|     AD_VP| 17000|          null|       100|           90|\n",
            "|        103|  Alexander|    Hunold| AHUNOLD|590.423.4567| 03.01.06|   IT_PROG|  9000|          null|       102|           60|\n",
            "|        104|      Bruce|     Ernst|  BERNST|590.423.4568| 21.05.07|   IT_PROG|  6000|          null|       103|           60|\n",
            "|        105|      David|    Austin| DAUSTIN|590.423.4569| 25.06.05|   IT_PROG|  4800|          null|       103|           60|\n",
            "|        106|      Valli| Pataballa|VPATABAL|590.423.4560| 05.02.06|   IT_PROG|  4800|          null|       103|           60|\n",
            "|        107|      Diana|   Lorentz|DLORENTZ|590.423.5567| 07.02.07|   IT_PROG|  4200|          null|       103|           60|\n",
            "|        108|      Nancy| Greenberg|NGREENBE|515.124.4569| 17.08.02|    FI_MGR| 12008|          null|       101|          100|\n",
            "|        109|     Daniel|    Faviet| DFAVIET|515.124.4169| 16.08.02|FI_ACCOUNT|  9000|          null|       108|          100|\n",
            "|        110|       John|      Chen|   JCHEN|515.124.4269| 28.09.05|FI_ACCOUNT|  8200|          null|       108|          100|\n",
            "|        111|     Ismael|   Sciarra|ISCIARRA|515.124.4369| 30.09.05|FI_ACCOUNT|  7700|          null|       108|          100|\n",
            "|        112|Jose Manuel|     Urman| JMURMAN|515.124.4469| 07.03.06|FI_ACCOUNT|  7800|          null|       108|          100|\n",
            "|        113|       Luis|      Popp|   LPOPP|515.124.4567| 07.12.07|FI_ACCOUNT|  6900|          null|       108|          100|\n",
            "|        114|        Den|  Raphaely|DRAPHEAL|515.127.4561| 07.12.02|    PU_MAN| 11000|          null|       100|           30|\n",
            "|        115|  Alexander|      Khoo|   AKHOO|515.127.4562| 18.05.03|  PU_CLERK|  3100|          null|       114|           30|\n",
            "|        116|     Shelli|     Baida|  SBAIDA|515.127.4563| 24.12.05|  PU_CLERK|  2900|          null|       114|           30|\n",
            "|        117|      Sigal|    Tobias| STOBIAS|515.127.4564| 24.07.05|  PU_CLERK|  2800|          null|       114|           30|\n",
            "|        118|        Guy|    Himuro| GHIMURO|515.127.4565| 15.11.06|  PU_CLERK|  2600|          null|       114|           30|\n",
            "|        119|      Karen|Colmenares|KCOLMENA|515.127.4566| 10.08.07|  PU_CLERK|  2500|          null|       114|           30|\n",
            "+-----------+-----------+----------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg1WvAVXGIuo"
      },
      "source": [
        "j_his = spark.read.csv('data/job_history.csv', header=True, sep='\\t')\n",
        "j_his.createOrReplaceTempView(\"job_history\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_toIKoNUJl6-",
        "outputId": "6c048354-afee-40c6-adf2-4c2c919e1cc7"
      },
      "source": [
        "j_his.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+----------+--------+----------+-------------+\n",
            "|EMPLOYEE_ID|START_DATE|END_DATE|    JOB_ID|DEPARTMENT_ID|\n",
            "+-----------+----------+--------+----------+-------------+\n",
            "|        102|  13.01.01|24.07.06|   IT_PROG|           60|\n",
            "|        101|  21.09.97|27.10.01|AC_ACCOUNT|          110|\n",
            "|        101|  28.10.01|15.03.05|    AC_MGR|          110|\n",
            "|        201|  17.02.04|19.12.07|    MK_REP|           20|\n",
            "|        114|  24.03.06|31.12.07|  ST_CLERK|           50|\n",
            "|        122|  01.01.07|31.12.07|  ST_CLERK|           50|\n",
            "|        200|  17.09.95|17.06.01|   AD_ASST|           90|\n",
            "|        176|  24.03.06|31.12.06|    SA_REP|           80|\n",
            "|        176|  01.01.07|31.12.07|    SA_MAN|           80|\n",
            "|        200|  01.07.02|31.12.06|AC_ACCOUNT|           90|\n",
            "+-----------+----------+--------+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v43K2fDAjUYb"
      },
      "source": [
        "Кто получает больше всего? Кто меньше всего?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0d8KF28PtFo"
      },
      "source": [
        "Больше всего"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm15EW8znWDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718f5e94-8219-46f5-9aab-f814526a196c"
      },
      "source": [
        "# код\n",
        "sqlDF = spark.sql(\"\"\"SELECT EMPLOYEE_ID, FIRST_NAME, LAST_NAME, SALARY \n",
        "                  FROM employees\n",
        "                  WHERE SALARY IN (SELECT MAX(CAST(SALARY AS INT))\n",
        "                                   FROM employees)\n",
        "                  \"\"\")\n",
        "sqlDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+----------+---------+------+\n",
            "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|SALARY|\n",
            "+-----------+----------+---------+------+\n",
            "|        100|    Steven|     King| 24000|\n",
            "+-----------+----------+---------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtTaCYLTPvRS"
      },
      "source": [
        "Меньше всего"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8afyJMn-PxS9",
        "outputId": "a9c20b8f-f87c-456e-daf6-80d7a6eb1ad5"
      },
      "source": [
        "# код\n",
        "sqlDF = spark.sql(\"\"\"SELECT EMPLOYEE_ID, FIRST_NAME, LAST_NAME, SALARY \n",
        "                  FROM employees\n",
        "                  WHERE SALARY IN (SELECT MIN(CAST(SALARY AS INT))\n",
        "                                   FROM employees)\n",
        "                  \"\"\")\n",
        "sqlDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+----------+---------+------+\n",
            "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|SALARY|\n",
            "+-----------+----------+---------+------+\n",
            "|        132|        TJ|    Olson|  2100|\n",
            "+-----------+----------+---------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg8-rkFlkMqm"
      },
      "source": [
        "Выведете топ 5 по зарплате."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMzJ_tJhnWuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ac2892-724e-4036-b3ce-84f3a58704f1"
      },
      "source": [
        "# код\n",
        "sqlDF = spark.sql(\"\"\"SELECT EMPLOYEE_ID, FIRST_NAME, LAST_NAME, CAST(SALARY as INT) as SALARY\n",
        "                     FROM employees\n",
        "                     ORDER BY SALARY DESC\n",
        "                     LIMIT 5\n",
        "\"\"\")\n",
        "sqlDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+----------+---------+------+\n",
            "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|SALARY|\n",
            "+-----------+----------+---------+------+\n",
            "|        100|    Steven|     King| 24000|\n",
            "|        101|     Neena|  Kochhar| 17000|\n",
            "|        102|       Lex|  De Haan| 17000|\n",
            "|        145|      John|  Russell| 14000|\n",
            "|        146|     Karen| Partners| 13500|\n",
            "+-----------+----------+---------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfBpDVYwkM7K"
      },
      "source": [
        "Сколько всего регионов? Сколько работников в каждом регионе?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJlLXiQXV5cK"
      },
      "source": [
        "Сколько всего регионов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2d2AiBknYSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae58a56-95b1-4ee8-a92c-beb7b6047aec"
      },
      "source": [
        "# код\n",
        "sqlDF = spark.sql(\"\"\"SELECT COUNT(*) AS counter_regions\n",
        "                     FROM (SELECT DISTINCT REGION_NAME\n",
        "                           FROM regions)\n",
        "\"\"\")\n",
        "sqlDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+\n",
            "|counter_regions|\n",
            "+---------------+\n",
            "|              4|\n",
            "+---------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QlFY9FIV8RN"
      },
      "source": [
        "Сколько работников в каждом регионе"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNdpZzuyV__8",
        "outputId": "8f18d8c1-f9d9-4d3d-ee2d-de0077036618"
      },
      "source": [
        "sqlDF = spark.sql(\"\"\"WITH cte AS(\n",
        "                          SELECT dep.DEPARTMENT_ID, loc.COUNTRY_ID\n",
        "                          FROM department AS dep INNER JOIN locations as loc\n",
        "                          ON dep.LOCATION_ID = loc.LOCATION_ID),\n",
        "                     cte2 AS(\n",
        "                          SELECT country.REGION_ID, cte.DEPARTMENT_ID\n",
        "                          FROM cte INNER JOIN country\n",
        "                          ON country.COUNTRY_ID = cte.COUNTRY_ID)\n",
        "\n",
        "                    SELECT regions.REGION_NAME, \n",
        "                    CASE WHEN counter_emp IS NULL\n",
        "                    THEN 0\n",
        "                    ELSE counter_emp\n",
        "                    END counter_emp\n",
        "                    FROM regions LEFT JOIN (\n",
        "                                                SELECT cte2.REGION_ID, count(emp.EMPLOYEE_ID) as counter_emp\n",
        "                                                FROM cte2 INNER JOIN employees AS emp\n",
        "                                                ON cte2.DEPARTMENT_ID = emp.DEPARTMENT_ID\n",
        "                                                GROUP BY cte2.REGION_ID) as t\n",
        "                    ON regions.REGION_ID = t.REGION_ID\n",
        "                    \"\"\")\n",
        "sqlDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----------+\n",
            "|         REGION_NAME|counter_emp|\n",
            "+--------------------+-----------+\n",
            "|                Asia|          0|\n",
            "|              Europe|         36|\n",
            "|Middle East and A...|          0|\n",
            "|            Americas|         70|\n",
            "+--------------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOY2unJ8kNXz"
      },
      "source": [
        "Выведете всех работников из Китая."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lhj9GAwnZgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c7b54a-c9ef-4bb0-da16-f8ece10b909e"
      },
      "source": [
        "# код\n",
        "sqlDF = spark.sql(\"\"\"WITH cte AS(\n",
        "                          SELECT dep.DEPARTMENT_ID, loc.COUNTRY_ID\n",
        "                          FROM department AS dep INNER JOIN locations as loc\n",
        "                          ON dep.LOCATION_ID = loc.LOCATION_ID)\n",
        "                    SELECT emp.EMPLOYEE_ID, emp.FIRST_NAME, emp.LAST_NAME, cte.COUNTRY_ID\n",
        "                    FROM cte INNER JOIN employees AS emp\n",
        "                    ON cte.DEPARTMENT_ID = emp.DEPARTMENT_ID\n",
        "                    WHERE cte.COUNTRY_ID = 'CN'\n",
        "                    \"\"\")\n",
        "sqlDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+----------+\n",
            "|DEPARTMENT_ID|COUNTRY_ID|\n",
            "+-------------+----------+\n",
            "+-------------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Nc8F-6kNR7"
      },
      "source": [
        "Укажите самую высокооплачиваемою должность."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZNAohM-naSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c8c1a3-3b15-4d04-ccac-10462996ee90"
      },
      "source": [
        "# код\n",
        "sqlDF = spark.sql(\"\"\"SELECT JOB_TITLE, MAX_SALARY\n",
        "                     FROM jobs\n",
        "                     WHERE MAX_SALARY IN (SELECT MAX(CAST(MAX_SALARY AS INT))\n",
        "                                          FROM jobs)\"\"\")\n",
        "sqlDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+----------+\n",
            "|JOB_TITLE|MAX_SALARY|\n",
            "+---------+----------+\n",
            "|President|     40000|\n",
            "+---------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzenJwUnkNL8"
      },
      "source": [
        "Выведете всех работников связанных с ИТ. Выведете их менеджеров. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eogYiLjXna3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "babc3927-aaa5-4848-9f27-ff06142bb344"
      },
      "source": [
        "# код\n",
        "sqlDF = spark.sql(\"\"\"SELECT EMPLOYEE_ID, FIRST_NAME, LAST_NAME, MANAGER_ID\n",
        "                     FROM employees\n",
        "                     WHERE JOB_ID = 'IT_PROG'\n",
        "\"\"\")\n",
        "sqlDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+----------+---------+----------+\n",
            "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|MANAGER_ID|\n",
            "+-----------+----------+---------+----------+\n",
            "|        103| Alexander|   Hunold|       102|\n",
            "|        104|     Bruce|    Ernst|       103|\n",
            "|        105|     David|   Austin|       103|\n",
            "|        106|     Valli|Pataballa|       103|\n",
            "|        107|     Diana|  Lorentz|       103|\n",
            "+-----------+----------+---------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvpCoeYPmLTW"
      },
      "source": [
        "Выведете имя и фамилию работника, его текущую и предыдущую должности и сколько полных недель и дней прошло с момент изменения. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkCvyzkVnbo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "169fa178-6f95-4123-a6f4-4725ccabbe17"
      },
      "source": [
        "# код\n",
        "sqlDF = spark.sql(\"\"\"WITH cte AS(\n",
        "                     SELECT jb.EMPLOYEE_ID, TO_DATE(CAST(UNIX_TIMESTAMP(emp.HIRE_DATE, 'dd.MM.yy') AS TIMESTAMP)) as START_DATE,\n",
        "                     TO_DATE(CAST(UNIX_TIMESTAMP(jb.END_DATE, 'dd.MM.yy') AS TIMESTAMP)) as END_DATE,\n",
        "                     jb.JOB_ID as PREV_JOB, emp.JOB_ID as NOW_JOB,\n",
        "                     emp.FIRST_NAME, emp.LAST_NAME,\n",
        "                     RANK() OVER(PARTITION BY jb.EMPLOYEE_ID ORDER BY END_DATE DESC) as RANK_JOB\n",
        "                     FROM job_history as jb JOIN employees as emp\n",
        "                     ON jb.EMPLOYEE_ID = emp.EMPLOYEE_ID\n",
        "                     ORDER BY jb.EMPLOYEE_ID),\n",
        "                     cte2 AS(\n",
        "                         SELECT *\n",
        "                         FROM cte\n",
        "                         WHERE RANK_JOB = 1 and START_DATE > END_DATE\n",
        "                     )\n",
        "\n",
        "                     SELECT FIRST_NAME, LAST_NAME, PREV_JOB, NOW_JOB, DATEDIFF(START_DATE, END_DATE) as DIFF_DAYS,\n",
        "                     CAST(DATEDIFF(START_DATE, END_DATE) / 30 AS INT) as DIFF_MONTHS FROM cte2\n",
        "\"\"\")\n",
        "#CAST(UNIX_TIMESTAMP(START_DATE, 'dd.MM.yy') AS TIMESTAMP)\n",
        "sqlDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---------+----------+-------+---------+-----------+\n",
            "|FIRST_NAME|LAST_NAME|  PREV_JOB|NOW_JOB|DIFF_DAYS|DIFF_MONTHS|\n",
            "+----------+---------+----------+-------+---------+-----------+\n",
            "|     Neena|  Kochhar|AC_ACCOUNT|  AD_VP|     1425|         47|\n",
            "+----------+---------+----------+-------+---------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOBhGAaanC0e"
      },
      "source": [
        "Выведете уникальные телефонные номера"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05BrXyrsncKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0151e9c0-7126-42df-cfaa-ed253dcb742f"
      },
      "source": [
        "# код\n",
        "sqlDF = spark.sql(\"\"\"SELECT DISTINCT PHONE_NUMBER\n",
        "                     FROM employees\n",
        "\"\"\")\n",
        "sqlDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|      PHONE_NUMBER|\n",
            "+------------------+\n",
            "|011.44.1344.429018|\n",
            "|      515.127.4566|\n",
            "|      515.127.4564|\n",
            "|011.44.1344.429278|\n",
            "|      515.123.4569|\n",
            "|      650.124.1434|\n",
            "|      650.123.2234|\n",
            "|011.44.1344.498718|\n",
            "|      650.127.1634|\n",
            "|      515.127.4561|\n",
            "|011.44.1345.629268|\n",
            "|      515.127.4562|\n",
            "|011.44.1644.429264|\n",
            "|011.44.1644.429262|\n",
            "|      650.501.1876|\n",
            "|      650.127.1834|\n",
            "|011.44.1343.529268|\n",
            "|011.44.1644.429265|\n",
            "|      515.123.8181|\n",
            "|      650.507.9833|\n",
            "+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdf1VBv3nMR-"
      },
      "source": [
        "Есть ли сотрудники с одинаковыми фамилиями и сколько их."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apTLX1o6jy6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbbaf942-f023-43ac-cc48-648cd376e154"
      },
      "source": [
        "# код\n",
        "sqlDF = spark.sql(\"\"\"SELECT LAST_NAME, COUNT(LAST_NAME) as COUNTER\n",
        "                     FROM employees\n",
        "                     GROUP BY LAST_NAME\n",
        "                     HAVING COUNT(LAST_NAME) > 1\n",
        "\"\"\")\n",
        "sqlDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-------+\n",
            "|LAST_NAME|COUNTER|\n",
            "+---------+-------+\n",
            "|    Smith|      2|\n",
            "|     King|      2|\n",
            "|Cambrault|      2|\n",
            "|   Taylor|      2|\n",
            "+---------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v1qE1reZKcw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}